{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "843e01aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from dataset import SegmentationDataset, get_dataloaders, DualCompose, DualResize, RandomRotationDual, RandomFlipDual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd91e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for images and masks\n",
    "# Ensure paired image/mask get identical random ops and fixed size\n",
    "target_size = (256, 256)\n",
    "\n",
    "joint_transform = DualCompose([\n",
    "    DualResize(target_size),  # Fixed size for batching\n",
    "    RandomRotationDual(degrees=15),\n",
    "    RandomFlipDual(flip_type=\"horizontal\", p=0.5),\n",
    "    RandomFlipDual(flip_type=\"vertical\", p=0.3),\n",
    "])\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "    # transforms.RandomApply([transforms.GaussianBlur(3, sigma=(0.1, 2.0))], p=0.2),\n",
    "    # transforms.RandomApply([transforms.RandomAdjustSharpness(2.0)], p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: (x > 0.5).float())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "310e4cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 720\n",
      "Validation samples: 180\n",
      "Evaluation samples: 379\n",
      "Total samples: 1279\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "dataset = SegmentationDataset(\n",
    "    images_dir='data/training/original',\n",
    "    masks_dir='data/training/gt',\n",
    "    joint_transform=joint_transform,\n",
    "    image_transform=image_transform,\n",
    "    mask_transform=mask_transform\n",
    ")\n",
    "eval_dataset = SegmentationDataset(\n",
    "    images_dir='data/testing/original',\n",
    "    masks_dir='data/testing/gt',\n",
    "    joint_transform=joint_transform,\n",
    "    image_transform=image_transform,\n",
    "    mask_transform=mask_transform\n",
    ")\n",
    "\n",
    "\n",
    "train_loader, val_loader = get_dataloaders(dataset, t_size=0.8, v_size=0.2, eval=False)\n",
    "eval_loader, _ = get_dataloaders(eval_dataset, t_size=1, v_size=0, eval=False)\n",
    "\n",
    "print(f\"Train samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
    "print(f\"Evaluation samples: {len(eval_loader.dataset)}\")\n",
    "print(f\"Total samples: {len(train_loader.dataset) + len(val_loader.dataset) + len(eval_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "730bc00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82717ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedUNet(nn.Module):\n",
    "    def __init__(self, img_size, num_classes, dropout=True, use_batch_norm=True):\n",
    "        super(ImprovedUNet, self).__init__()\n",
    "\n",
    "        dropout_rate = 0.2 if dropout else 0\n",
    "        \n",
    "        # Encoder (Contracting Path)\n",
    "        self.enc1 = EncoderBlock(3, 64, dropout_rate)       # 256x256 -> 128x128\n",
    "        self.enc2 = EncoderBlock(64, 128, dropout_rate)     # 128x128 -> 64x64\n",
    "        self.enc3 = EncoderBlock(128, 256, dropout_rate)    # 64x64 -> 32x32\n",
    "        self.enc4 = EncoderBlock(256, 512, dropout_rate)    # 32x32 -> 16x16\n",
    "        self.enc5 = EncoderBlock(512, 1024, dropout_rate)   # 16x16 -> 8x8\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = EncoderBlock(512, 1024) # 16x16 -> 8x8\n",
    "        # self.bottleneck = EncoderBlock(1024, 2048) # 16x16 -> 8x8\n",
    "        \n",
    "        # Decoder (Expanding Path) with skip connections\n",
    "        self.dec5 = DecoderBlock(2048, 1024, dropout_rate)  # 4x4 -> 8x8 (skip from enc5)\n",
    "        self.dec4 = DecoderBlock(1024, 512, dropout_rate)   # 8x8 -> 16x16 (skip from enc4)\n",
    "        self.dec3 = DecoderBlock(512, 256, dropout_rate)    # 16x16 -> 32x32 (skip from enc3)\n",
    "        self.dec2 = DecoderBlock(256, 128, dropout_rate)    # 32x32 -> 64x64 (skip from enc2)\n",
    "        self.dec1 = DecoderBlock(128, 64, dropout_rate)     # 64x64 -> 128x128 (skip from enc1)\n",
    "        \n",
    "        # Final upsampling to original size\n",
    "        self.final_upsample = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.final_conv = nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        e1, skip1 = self.enc1(x)      # 256x256 -> 128x128, 64 channels\n",
    "        e2, skip2 = self.enc2(e1)    # 128x128 -> 64x64, 128 channels\n",
    "        e3, skip3 = self.enc3(e2)    # 64x64 -> 32x32, 256 channels\n",
    "        e4, skip4 = self.enc4(e3)    # 32x32 -> 16x16, 512 channels\n",
    "        # e5, skip5 = self.enc5(e4)    # 16x16 -> 8x8, 1024 channels\n",
    "        \n",
    "        # Bottleneck\n",
    "        b, _ = self.bottleneck(e4)    # 8x8 -> 4x4, 2048 channels\n",
    "        \n",
    "        # Decoder path with skip connections\n",
    "        # d5 = self.dec5(b, skip5)      # 4x4 -> 8x8, 1024 channels\n",
    "        d4 = self.dec4(d4, skip4)      # 8x8 -> 16x16, 512 channels\n",
    "        d3 = self.dec3(d4, skip3)      # 16x16 -> 32x32, 256 channels\n",
    "        d2 = self.dec2(d3, skip2)      # 32x32 -> 64x64, 128 channels\n",
    "        d1 = self.dec1(d2, skip1)      # 64x64 -> 128x128, 64 channels\n",
    "        \n",
    "        # Final upsampling and classification\n",
    "        x = self.final_upsample(d1)   # 128x128 -> 256x256, 32 channels\n",
    "        x = self.final_conv(x)        # 256x256 -> 256x256, 1 channel\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.2, use_batch_norm=True):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        norm_layer = nn.BatchNorm2d if use_batch_norm else nn.InstanceNorm2d\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.in1 = norm_layer(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.in2 = norm_layer(out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.in1(self.conv1(x)))\n",
    "        x = F.relu(self.in2(self.conv2(x)))\n",
    "        skip = x  # Store for skip connection\n",
    "        x = self.pool(x)\n",
    "        if self.dropout_rate > 0:\n",
    "            x = F.dropout2d(x, p=self.dropout_rate, training=self.training)\n",
    "        return x, skip\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.2, use_batch_norm=True):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "        norm_layer = nn.BatchNorm2d if use_batch_norm else nn.InstanceNorm2d\n",
    "        # Upsampling\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        \n",
    "        # Convolutions after concatenation - Fixed input channels\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.in1 = norm_layer(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.in2 = norm_layer(out_channels)\n",
    "        \n",
    "    def forward(self, x, skip):\n",
    "        # Upsample\n",
    "        x = self.up(x)\n",
    "        \n",
    "        # Ensure skip connection has the same spatial dimensions\n",
    "        if x.shape[2:] != skip.shape[2:]:\n",
    "            skip = F.interpolate(skip, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Concatenate with skip connection\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        \n",
    "        # Convolutions\n",
    "        x = F.relu(self.in1(self.conv1(x)))\n",
    "        x = F.relu(self.in2(self.conv2(x)))\n",
    "        if self.dropout_rate > 0:\n",
    "            x = F.dropout2d(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a21ea084",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = target_size \n",
    "num_classes = 1         # Single channel output for binary segmentation (0 or 1)\n",
    "model = ImprovedUNet(img_size, num_classes, dropout=False, use_batch_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92e614fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage as ndi\n",
    "\n",
    "def _mask_edges(mask: np.ndarray) -> np.ndarray:\n",
    "    mask = mask.astype(bool)\n",
    "    if mask.sum() == 0:\n",
    "        return mask\n",
    "    footprint = np.ones((3, 3), dtype=bool)\n",
    "    eroded = ndi.binary_erosion(mask, structure=footprint)\n",
    "    return np.logical_xor(mask, eroded)\n",
    "\n",
    "def hausdorff(pred: np.ndarray, gt: np.ndarray, percentile=95, spacing=(1.0, 1.0)) -> float:\n",
    "    pred_bin = pred > 0.5\n",
    "    gt_bin = gt > 0.5\n",
    "    if pred_bin.sum() == 0 and gt_bin.sum() == 0:\n",
    "        return 0.0\n",
    "    if pred_bin.sum() == 0 or gt_bin.sum() == 0:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    pred_edge = _mask_edges(pred_bin)\n",
    "    gt_edge = _mask_edges(gt_bin)\n",
    "\n",
    "    if pred_edge.sum() == 0 and gt_edge.sum() == 0:\n",
    "        return 0.0\n",
    "    if pred_edge.sum() == 0 or gt_edge.sum() == 0:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    dt_pred = ndi.distance_transform_edt(~pred_edge, sampling=spacing)\n",
    "    dt_gt = ndi.distance_transform_edt(~gt_edge, sampling=spacing)\n",
    "\n",
    "    dists = np.concatenate([dt_pred[gt_edge], dt_gt[pred_edge]])\n",
    "    if dists.size == 0:\n",
    "        return 0.0\n",
    "    return float(np.percentile(dists, percentile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67d14988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function and optimizer defined:\n",
      "Loss: BCELoss()\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.5, dice_weight=0.5, smooth=1e-6, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.smooth = smooth\n",
    "        self.reduction = reduction\n",
    "        self.bce = nn.BCELoss(reduction=reduction)  # model outputs probs (sigmoid), so BCELoss is correct\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        # preds, targets: N x 1 x H x W, preds are probabilities in [0,1]\n",
    "        preds = preds.clamp(min=self.smooth, max=1.0 - self.smooth)\n",
    "\n",
    "        # BCE\n",
    "        bce_loss = self.bce(preds, targets)\n",
    "\n",
    "        # Dice (computed per-sample, then averaged)\n",
    "        dims = (1, 2, 3)\n",
    "        intersection = (preds * targets).sum(dims)\n",
    "        preds_sum = preds.sum(dims)\n",
    "        targets_sum = targets.sum(dims)\n",
    "        dice = (2.0 * intersection + self.smooth) / (preds_sum + targets_sum + self.smooth)\n",
    "        dice_loss = 1.0 - dice\n",
    "        if self.reduction == 'mean':\n",
    "            dice_loss = dice_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            dice_loss = dice_loss.sum()\n",
    "\n",
    "        return self.bce_weight * bce_loss + self.dice_weight * dice_loss\n",
    "\n",
    "class BCEHousdorffLoss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.5, smooth=1e-6, percentile=95, eps=1e-6, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.smooth = smooth\n",
    "        self.percentile = percentile\n",
    "        self.eps = eps\n",
    "        self.bce = nn.BCELoss(reduction=reduction)  # model outputs probs (sigmoid), so BCELoss is correct\n",
    "\n",
    "\n",
    "    def hausdorff(self, preds, targets):\n",
    "        # This Hausdorff loss calculation differs from the previously implemented hausdorff function in two main ways:\n",
    "        # 1. It computes a *soft* Hausdorff-like loss by weighting per-pixel errors with distances from the ground truth \n",
    "        #    boundaries, rather than thresholding predictions and computing Hausdorff directly on binary masks.\n",
    "        # 2. Instead of using direct percentile distances between predicted and ground truth edges, it selects the top-k\n",
    "        #    weighted error pixels (determined by the given percentile) and averages them to provide a differentiable estimate \n",
    "        #    that is friendly to optimization, making it suitable as a training loss.\n",
    "\n",
    "        # preds, targets: [N, 1, H, W], preds in [0,1]\n",
    "        preds = preds.clamp(self.eps, 1.0 - self.eps)\n",
    "        N = preds.shape[0]\n",
    "        losses = []\n",
    "        for n in range(N):\n",
    "            y = (targets[n, 0] > 0.5).detach().cpu().numpy().astype(bool)\n",
    "            # Distance to fg and bg (robust boundary emphasis)\n",
    "            dt_fg = ndi.distance_transform_edt(~y)\n",
    "            dt_bg = ndi.distance_transform_edt(y)\n",
    "            dist = torch.from_numpy(dt_fg + dt_bg).to(preds.device, dtype=preds.dtype)\n",
    "\n",
    "            err = (preds[n, 0] - targets[n, 0]).abs()         # per-pixel soft error\n",
    "            weighted = err * dist                              # HD-like weighting\n",
    "            flat = weighted.flatten()\n",
    "\n",
    "            k = max(1, int(np.ceil(flat.numel() * (self.percentile / 100.0))))\n",
    "            topk_vals, _ = torch.topk(flat, k, largest=True)\n",
    "            losses.append(topk_vals.mean())\n",
    "        return torch.stack(losses).mean()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        # preds, targets: N x 1 x H x W, preds are probabilities in [0,1]\n",
    "        preds = preds.clamp(min=self.smooth, max=1.0 - self.smooth)\n",
    "\n",
    "        # BCE\n",
    "        bce_loss = self.bce(preds, targets)\n",
    "\n",
    "        # Hausdorff distance\n",
    "        hausdorff = self.hausdorff(preds, targets)\n",
    "\n",
    "        return self.bce_weight * bce_loss + 0.5 * hausdorff\n",
    "\n",
    "\n",
    "# Binary Cross Entropy Loss for segmentation\n",
    "criterion = nn.BCELoss()\n",
    "# criterion = BCEDiceLoss()\n",
    "# criterion = BCEHousdorffLoss()\n",
    "# Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)#, weight_decay=0.0001)\n",
    "\n",
    "print(\"Loss function and optimizer defined:\")\n",
    "print(f\"Loss: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "541b6eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with accuracy tracking\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device='cpu'):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for images, masks in dataloader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "# Validation function with comprehensive metrics tracking\n",
    "def validate_epoch(model, dataloader, criterion, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_iou = 0.0\n",
    "    total_dice = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            pred_binary = (outputs > 0.5).float()\n",
    "            \n",
    "            # Accuracy\n",
    "            accuracy = (pred_binary == masks).float().mean()\n",
    "            total_accuracy += accuracy.item()\n",
    "            \n",
    "            # IoU (Intersection over Union)\n",
    "            intersection = (pred_binary * masks).sum(dim=(1, 2, 3))  # Sum over spatial dimensions\n",
    "            union = (pred_binary + masks - pred_binary * masks).sum(dim=(1, 2, 3))\n",
    "            iou = (intersection / (union + 1e-8)).mean()  # Average IoU across batch\n",
    "            total_iou += iou.item()\n",
    "            \n",
    "            # Dice Coefficient\n",
    "            dice = (2 * intersection / (pred_binary.sum(dim=(1, 2, 3)) + masks.sum(dim=(1, 2, 3)) + 1e-8)).mean()\n",
    "            total_dice += dice.item()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    return (total_loss / num_batches, \n",
    "            total_accuracy / num_batches, \n",
    "            total_iou / num_batches, \n",
    "            total_dice / num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95d47c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Starting training with convergence detection...\n",
      "Max epochs: 50, Patience: 8, Min delta: 0.001\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [2048, 1024, 3, 3], expected input[8, 512, 16, 16] to have 1024 channels, but got 512 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 41\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax epochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Patience: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatience\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Min delta: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_delta\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epochs):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     val_loss, val_accuracy, val_iou, val_dice \u001b[38;5;241m=\u001b[39m validate_epoch(model, val_loader, criterion, device)\n",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, masks)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MAI/UB/DLMIA/P1/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MAI/UB/DLMIA/P1/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m, in \u001b[0;36mImprovedUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m e4, skip4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc4(e3)    \u001b[38;5;66;03m# 32x32 -> 16x16, 512 channels\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# e5, skip5 = self.enc5(e4)    # 16x16 -> 8x8, 1024 channels\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Bottleneck\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m b, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbottleneck\u001b[49m\u001b[43m(\u001b[49m\u001b[43me4\u001b[49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# 8x8 -> 4x4, 2048 channels\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Decoder path with skip connections\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# d5 = self.dec5(b, skip5)      # 4x4 -> 8x8, 1024 channels\u001b[39;00m\n\u001b[1;32m     41\u001b[0m d4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec4(d4, skip4)      \u001b[38;5;66;03m# 8x8 -> 16x16, 512 channels\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MAI/UB/DLMIA/P1/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MAI/UB/DLMIA/P1/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 69\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 69\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     70\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m     71\u001b[0m     skip \u001b[38;5;241m=\u001b[39m x  \u001b[38;5;66;03m# Store for skip connection\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MAI/UB/DLMIA/P1/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MAI/UB/DLMIA/P1/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/MAI/UB/DLMIA/P1/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:548\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MAI/UB/DLMIA/P1/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:543\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    533\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    534\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    542\u001b[0m     )\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [2048, 1024, 3, 3], expected input[8, 512, 16, 16] to have 1024 channels, but got 512 channels instead"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device(\"mps\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "force_train = True\n",
    "validation_metric = \"iou\"\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "model_file = pathlib.Path(\"improved_segmentation_model1.pth\")\n",
    "if model_file.exists() and not force_train:\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    print(f\"Model loaded from {model_file}\")\n",
    "else:\n",
    "    # Training parameters with convergence detection\n",
    "    max_epochs = 50  # Increased max epochs\n",
    "    patience = 8     # Number of epochs to wait for improvement\n",
    "    min_delta = 0.001  # Minimum change to qualify as improvement\n",
    "    \n",
    "    # Convergence tracking variables\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_accuracy = 0.0\n",
    "    best_val_iou = 0.0\n",
    "    best_val_dice = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    early_stop = False\n",
    "    \n",
    "    # Lists to track training history for convergence analysis\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_dice_scores = []\n",
    "    val_iou_scores = []\n",
    "\n",
    "    print(\"Starting training with convergence detection...\")\n",
    "    print(f\"Max epochs: {max_epochs}, Patience: {patience}, Min delta: {min_delta}\")\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # Training\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_accuracy, val_iou, val_dice = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Store metrics for convergence analysis\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_dice_scores.append(val_dice)\n",
    "        val_iou_scores.append(val_iou)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{max_epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"  Val IoU: {val_iou:.4f}, Val Dice: {val_dice:.4f}\")\n",
    "        \n",
    "        # Check for improvement (using Dice coefficient as primary metric)\n",
    "        if validation_metric == \"dice\":\n",
    "            improvement = val_dice > best_val_dice + min_delta\n",
    "            val_name = \"Dice\"\n",
    "        elif validation_metric == \"accuracy\":\n",
    "            improvement = val_accuracy > best_val_accuracy + min_delta\n",
    "            val_name = \"Accuracy\"\n",
    "        elif validation_metric == \"iou\":\n",
    "            improvement = val_iou > best_val_iou + min_delta\n",
    "            val_name = \"IoU\"\n",
    "        if improvement:\n",
    "            # Save best model\n",
    "            best_val_iou = val_iou\n",
    "            best_val_dice = val_dice\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save(model.state_dict(), 'best_segmentation_model.pth')\n",
    "            print(f\"  New best model saved! ({val_name}: {val_iou:.4f})\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"  No improvement for {epochs_without_improvement} epochs\")\n",
    "        \n",
    "        # Check for convergence/early stopping\n",
    "        if epochs_without_improvement >= patience:\n",
    "            early_stop = True\n",
    "            print(f\"\\n Early stopping triggered!\")\n",
    "            print(f\"   No improvement in {val_name} score for {patience} epochs\")\n",
    "            print(f\"   Best {val_name} score: {best_val_iou:.4f}\")\n",
    "            break\n",
    "        \n",
    "        # Check for convergence based on loss stability\n",
    "        if epoch >= 5:  # Need at least 5 epochs to check convergence\n",
    "            recent_losses = val_losses[-5:]  # Last 5 epochs\n",
    "            loss_std = np.std(recent_losses)\n",
    "            loss_mean = np.mean(recent_losses)\n",
    "            cv = loss_std / (loss_mean + 1e-8)  # Coefficient of variation\n",
    "            \n",
    "            if cv < 0.01:  # Very stable loss (CV < 1%)\n",
    "                print(f\"   Loss convergence detected (CV: {cv:.4f})\")\n",
    "                if epochs_without_improvement >= 3:  # Also need some patience\n",
    "                    early_stop = True\n",
    "                    print(f\"\\n Convergence-based early stopping!\")\n",
    "                    print(f\"   Loss coefficient of variation: {cv:.4f}\")\n",
    "                    break\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    if not early_stop:\n",
    "        print(\"Training completed - reached maximum epochs!\")\n",
    "    \n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"  Total epochs trained: {epoch + 1}\")\n",
    "    print(f\"  Best validation metrics:\")\n",
    "    print(f\"    IoU: {best_val_iou:.4f}\")\n",
    "    print(f\"    Dice: {best_val_dice:.4f}\")\n",
    "    print(f\"    Accuracy: {best_val_accuracy:.4f}\")\n",
    "    print(f\"    Loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933386e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_predictions(model, dataloader, num_images=5, device='cpu'):\n",
    "    \"\"\"\n",
    "    Visualize model predictions on evaluation set\n",
    "    \n",
    "    Args:\n",
    "        model: Trained U-Net model\n",
    "        dataloader: Evaluation dataloader\n",
    "        num_images: Number of images to display\n",
    "        device: Device to run inference on\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get first num_images from dataloader\n",
    "    images_to_show = []\n",
    "    masks_to_show = []\n",
    "    predictions_to_show = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (image, mask) in enumerate(dataloader):\n",
    "            # Move to device\n",
    "            image = image.to(device)\n",
    "            \n",
    "            # Get prediction\n",
    "            outputs = model(image)\n",
    "            prediction = (outputs > 0.5).float()\n",
    "\n",
    "            if i >= num_images:\n",
    "                pred_np = prediction[0].cpu().numpy().squeeze()\n",
    "                predictions_to_show.append(pred_np)\n",
    "                continue\n",
    "            \n",
    "            # Convert to numpy for visualization\n",
    "            image_np = image[0].cpu().numpy().transpose(1, 2, 0)\n",
    "            mask_np = mask[0].cpu().numpy().squeeze()\n",
    "            pred_np = prediction[0].cpu().numpy().squeeze()\n",
    "            \n",
    "            # Denormalize image (reverse ImageNet normalization)\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            image_np = image_np * std + mean\n",
    "            image_np = np.clip(image_np, 0, 1)\n",
    "            \n",
    "            images_to_show.append(image_np)\n",
    "            masks_to_show.append(mask_np)\n",
    "            predictions_to_show.append(pred_np)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(num_images, 3, figsize=(15, 5*num_images))\n",
    "    if num_images == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Original image\n",
    "        axes[i, 0].imshow(images_to_show[i])\n",
    "        axes[i, 0].set_title(f'Original Image {i+1}')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Ground truth mask\n",
    "        axes[i, 1].imshow(masks_to_show[i], cmap='gray')\n",
    "        axes[i, 1].set_title(f'Ground Truth {i+1}')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Prediction\n",
    "        axes[i, 2].imshow(predictions_to_show[i], cmap='gray')\n",
    "        axes[i, 2].set_title(f'Prediction {i+1}')\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return images_to_show, masks_to_show, predictions_to_show\n",
    "\n",
    "def calculate_metrics(prediction, ground_truth, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate segmentation metrics\n",
    "    \n",
    "    Args:\n",
    "        prediction: Model prediction (0-1)\n",
    "        ground_truth: Ground truth mask (0-1)\n",
    "        threshold: Threshold for binary prediction\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing metrics\n",
    "    \"\"\"\n",
    "    # Convert to binary\n",
    "    pred_binary = (prediction > threshold).astype(np.uint8)\n",
    "    gt_binary = ground_truth.astype(np.uint8)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    intersection = np.logical_and(pred_binary, gt_binary).sum()\n",
    "    union = np.logical_or(pred_binary, gt_binary).sum()\n",
    "    \n",
    "    # IoU (Jaccard Index)\n",
    "    iou = intersection / (union + 1e-8)\n",
    "    \n",
    "    # Dice Coefficient\n",
    "    dice = (2 * intersection) / (pred_binary.sum() + gt_binary.sum() + 1e-8)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = np.mean(pred_binary == gt_binary)\n",
    "\n",
    "    # Confusion matrix components\n",
    "    tp = np.logical_and(pred_binary == 1, gt_binary == 1).sum()\n",
    "    tn = np.logical_and(pred_binary == 0, gt_binary == 0).sum()\n",
    "    fp = np.logical_and(pred_binary == 1, gt_binary == 0).sum()\n",
    "    fn = np.logical_and(pred_binary == 0, gt_binary == 1).sum()\n",
    "\n",
    "    # Sensitivity (Recall) and Specificity\n",
    "    sensitivity = tp / (tp + fn + 1e-8)\n",
    "    specificity = tn / (tn + fp + 1e-8)\n",
    "\n",
    "    # Hausdorff distance\n",
    "    h = hausdorff(pred_binary, gt_binary)\n",
    "    \n",
    "    return {\n",
    "        'IoU': iou,\n",
    "        'Dice': dice,\n",
    "        'Accuracy': accuracy,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'Hausdorff': h\n",
    "    }\n",
    "\n",
    "print(\"Visualization functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21c21e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model (if you want to load from saved checkpoint)\n",
    "# model.load_state_dict(torch.load('best_segmentation_model.pth'))\n",
    "\n",
    "# Visualize predictions on evaluation set\n",
    "print(\"Visualizing predictions on evaluation set...\")\n",
    "images, masks, predictions = visualize_predictions(\n",
    "    model, eval_loader, num_images=5, device=device\n",
    ")\n",
    "\n",
    "# Calculate metrics for each prediction\n",
    "print(\"\\nCalculating metrics for each prediction:\")\n",
    "# Calculate average metrics\n",
    "avg_iou = np.mean([calculate_metrics(pred, mask)['IoU'] for pred, mask in zip(predictions, masks)])\n",
    "avg_dice = np.mean([calculate_metrics(pred, mask)['Dice'] for pred, mask in zip(predictions, masks)])\n",
    "avg_acc = np.mean([calculate_metrics(pred, mask)['Accuracy'] for pred, mask in zip(predictions, masks)])\n",
    "avg_sens = np.mean([calculate_metrics(pred, mask)['Sensitivity'] for pred, mask in zip(predictions, masks)])\n",
    "avg_spec = np.mean([calculate_metrics(pred, mask)['Specificity'] for pred, mask in zip(predictions, masks)])\n",
    "avg_hausdorff = np.mean([calculate_metrics(pred, mask)['Hausdorff'] for pred, mask in zip(predictions, masks)])\n",
    "\n",
    "print(f\"Average Metrics:\")\n",
    "print(f\"  Sensitivity: {avg_sens:.4f}\")\n",
    "print(f\"  Specificity: {avg_spec:.4f}\")\n",
    "print(f\"  Accuracy: {avg_acc:.4f}\")\n",
    "print(f\"  IoU: {avg_iou:.4f}\")\n",
    "print(f\"  Dice: {avg_dice:.4f}\")\n",
    "print(f\"  Hausdorff: {avg_hausdorff:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
